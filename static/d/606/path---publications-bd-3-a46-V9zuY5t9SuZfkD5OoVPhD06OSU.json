{"data":{"allPublicationsJson":{"nodes":[{"title":"Folding-based compression of point cloud attributes","abstract":"Existing techniques to compress point cloud attributes leverage either geometric or video-based compression tools. In this work, we explore a radically different approach inspired by recent advances in point cloud representation learning. A point cloud can be interpreted as a 2D manifold in a 3D space. As that, its attributes could be mapped onto a folded 2D grid; compressed through a conventional 2D image codec; and mapped back at the decoder side to recover attributes on 3D points. The folding operation is optimized by employing a deep neural network as a parametric folding function. As mapping is lossy in nature, we propose several strategies to refine it in such a way that attributes in 3D can be mapped to the 2D grid with minimal distortion. This approach can be flexibly applied to portions of point clouds in order to better adapt to local geometric complexity, and thus has a potential for being used as a tool in existing or future coding pipelines. Our preliminary results show that the proposed folding-based coding scheme can already reach performance similar to the latest MPEG GPCC codec.","URL":"http://arxiv.org/abs/2002.04439","author":[{"family":"Quach","given":"Maurice"},{"family":"Valenzise","given":"Giuseppe"},{"family":"Dufaux","given":"Frederic"}],"issued":{"date-parts":[["2020",2,11]]},"fields":{"issued":"2020-02-11","custom":{"links":[{"name":"arXiv","URL":"http://arxiv.org/abs/2002.04439"}],"image":{"src":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAC4jAAAuIwF4pT92AAAB+0lEQVQoz02R3U+SARjF+bf6G6qbuqjVSFOBMPSlWCkL1MytFmWbLcdsw1h2w1KklcyVmQT0gjMJXmooiB+lM/nIouL718vLYp6b5zzn4uw551EhIywG8D6+3aBUKhVlvn41Q3h2UuHlckmZ085xNsR5hdeqVYqVKs4Hg+TTkqJVZU3VIKJ/kZ4Tx0hvbPEfjvH7XD9/kp+FYksbu9XHwxtGitWWhEV7Bpd9rLWrDg9/0N9voaNdh1WrJhZ6Tyq9Sbe+F12XnqHui6S+SKx8XMVkMnPVIDBqFjjM5/B4XqDT9WLUdDFpG+JXoYAqkz2gra0T9bl2rKbLTD1xIMXjtF/QYBKuoD59HLvdjj/gR39JYMBsZuzOTYKBAE+nptB0Gugz9nBvZIBoLN6MHJfiuFzP+VMqt05/s7DA3JxXjvxb7rCpz856EENh/pZKctdlJEnCdteGa3qGnZ1t1j5HUdXrdY6isddrNYJLb7FYhxg0dBBa9CoGDvsjenuMcg1q1qRVIsshnjkdCMZraM+ewjUx2rywJhscNS4ViwR9iyy982E0GPgQ8Cna/EuPHH8CoVtPcj3BeiJB9FMEURQZHh7B7XY3DRvI5nJkstnW+9ObW+TyeSLRKPvfDyjIha+tJ9nd22N5ZYXtna8kUym+7e5ykMnIvUvE5Ar+AYW5DZPvcrQAAAAAAElFTkSuQmCC","aspectRatio":2.5419161676646707,"src":"/static/9f720b3fa5032763a50cea91fbb4498f/94286/2020_02_pcc_folding.png","srcSet":"/static/9f720b3fa5032763a50cea91fbb4498f/ed021/2020_02_pcc_folding.png 200w,\n/static/9f720b3fa5032763a50cea91fbb4498f/4e842/2020_02_pcc_folding.png 400w,\n/static/9f720b3fa5032763a50cea91fbb4498f/94286/2020_02_pcc_folding.png 800w,\n/static/9f720b3fa5032763a50cea91fbb4498f/d8210/2020_02_pcc_folding.png 1200w,\n/static/9f720b3fa5032763a50cea91fbb4498f/ab3db/2020_02_pcc_folding.png 1600w,\n/static/9f720b3fa5032763a50cea91fbb4498f/a57a0/2020_02_pcc_folding.png 1698w","srcWebp":"/static/9f720b3fa5032763a50cea91fbb4498f/64c2a/2020_02_pcc_folding.webp","srcSetWebp":"/static/9f720b3fa5032763a50cea91fbb4498f/bc926/2020_02_pcc_folding.webp 200w,\n/static/9f720b3fa5032763a50cea91fbb4498f/14fee/2020_02_pcc_folding.webp 400w,\n/static/9f720b3fa5032763a50cea91fbb4498f/64c2a/2020_02_pcc_folding.webp 800w,\n/static/9f720b3fa5032763a50cea91fbb4498f/f9d14/2020_02_pcc_folding.webp 1200w,\n/static/9f720b3fa5032763a50cea91fbb4498f/2e3ee/2020_02_pcc_folding.webp 1600w,\n/static/9f720b3fa5032763a50cea91fbb4498f/07edb/2020_02_pcc_folding.webp 1698w","sizes":"(max-width: 800px) 100vw, 800px","presentationWidth":800}}}},"publishedArticle":null}}},{"title":"Learning Convolutional Transforms for Lossy Point Cloud Geometry Compression","abstract":"Efficient point cloud compression is fundamental to enable the deployment of virtual and mixed reality applications, since the number of points to code can range in the order of millions. In this paper, we present a novel data-driven geometry compression method for static point clouds based on learned convolutional transforms and uniform quantization. We perform joint optimization of both rate and distortion using a trade-off parameter. In addition, we cast the decoding process as a binary classification of the point cloud occupancy map. Our method outperforms the MPEG reference solution in terms of rate-distortion on the Microsoft Voxelized Upper Bodies dataset with 51.5% BDBR savings on average. Moreover, while octree-based methods face exponential diminution of the number of points at low bitrates, our method still produces high resolution outputs even at low bitrates.","URL":"http://arxiv.org/abs/1903.08548","author":[{"family":"Quach","given":"Maurice"},{"family":"Valenzise","given":"Giuseppe"},{"family":"Dufaux","given":"Frederic"}],"issued":{"date-parts":[["2019",3,20]]},"fields":{"issued":"2019-03-20","custom":{"links":[{"name":"arXiv","URL":"https://arxiv.org/abs/1903.08548"},{"name":"Source code","URL":"https://github.com/mauriceqch/pcc_geo_cnn"},{"name":"Supplementary Material Website","URL":"https://mauriceqch.github.io/pcc_geo_cnn_samples/"},{"name":"Supplementary Material","URL":"https://github.com/mauriceqch/pcc_geo_cnn_samples"}],"image":{"src":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAC4jAAAuIwF4pT92AAACFUlEQVQoz42MbUuTYRiG7woE0aboSMpqGmFpkZGlaBjL1Mx8mU0fdVOn23z2bLpNCSGzpA+SCllQ/6EfZnt3Pntz1hc9ujczCvrQDSfneV3HeV8ims4Tz34nLD2WOZSSs/REVuZfu3DqZPcXy5ywSOqguCvc0Q9+IM5OvKfGtsa8Os1V9zaGmW1q1Y80Br5Q7tihfsyP5nNhdH6g0rHFFc8nbvo/UypZs3UOl0+j0rlDqW2Dwc2vCNFmp9xsw9XfSoPFi+h0Ynjmpda6hDC7MXZaUIcfcWFgUbJZKvp9XHoRlGyehg4zvrGnVA0GEK02upa35cFr7fQpTryeIO8Cbh70DlDeNkxdzyQlzd1Yp1QWtQBvF2e5YX6Ood3C5SfjVLT0MjWjElA9rC/YMd7v5rFzRR4UZ7h17yHKqJ27dxopq65BVJs4X38bcc5AW0cX1hGFpsbrlBovIox1lJmaECWV9PVZsAwO09RgQpRV0dIzhLCMKrg1H6uv15h2a4zZHbhUjYXAEuP2GbQFP69W17DNzaNMSebx4vMHUWzTLL9cYfXNOhMONyPKJBubW4hUSkfXdRKJBPr+Pmk5Z9JpkskkBZZM7p1k2dH3T1320ikS8bj8F2dPKpfLks/nEbu73wiHw0Qikd8ejUaLHgqFivnPORaLEZK9QjeTyZDNZouey+XIHx4ijo+PKejo6IjT/L/61/sJ2iPsw4c385gAAAAASUVORK5CYII=","aspectRatio":2.3529411764705883,"src":"/static/9eac4f202f7ec4d87cf376538e329129/94286/2019_pcc_geo_cnn.png","srcSet":"/static/9eac4f202f7ec4d87cf376538e329129/ed021/2019_pcc_geo_cnn.png 200w,\n/static/9eac4f202f7ec4d87cf376538e329129/4e842/2019_pcc_geo_cnn.png 400w,\n/static/9eac4f202f7ec4d87cf376538e329129/94286/2019_pcc_geo_cnn.png 800w,\n/static/9eac4f202f7ec4d87cf376538e329129/d8210/2019_pcc_geo_cnn.png 1200w,\n/static/9eac4f202f7ec4d87cf376538e329129/6fc85/2019_pcc_geo_cnn.png 1280w","srcWebp":"/static/9eac4f202f7ec4d87cf376538e329129/64c2a/2019_pcc_geo_cnn.webp","srcSetWebp":"/static/9eac4f202f7ec4d87cf376538e329129/bc926/2019_pcc_geo_cnn.webp 200w,\n/static/9eac4f202f7ec4d87cf376538e329129/14fee/2019_pcc_geo_cnn.webp 400w,\n/static/9eac4f202f7ec4d87cf376538e329129/64c2a/2019_pcc_geo_cnn.webp 800w,\n/static/9eac4f202f7ec4d87cf376538e329129/f9d14/2019_pcc_geo_cnn.webp 1200w,\n/static/9eac4f202f7ec4d87cf376538e329129/0c6c7/2019_pcc_geo_cnn.webp 1280w","sizes":"(max-width: 800px) 100vw, 800px","presentationWidth":800}}}},"publishedArticle":{"event":"2019 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2019.8803413"}}}}]}},"pageContext":{"isCreatedByStatefulCreatePages":true}}