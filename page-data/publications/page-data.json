{"componentChunkName":"component---src-pages-publications-js","path":"/publications/","result":{"data":{"allPublicationsJson":{"nodes":[{"title":"Learning-based lossless compression of 3D point cloud geometry","abstract":"This paper presents a learning-based, lossless compression method for static point cloud geometry, based on context-adaptive arithmetic coding. Unlike most existing methods working in the octree domain, our encoder operates in a hybrid mode, mixing octree and voxel-based coding. We adaptively partition the point cloud into multi-resolution voxel blocks according to the point cloud structure and use octree to signal the partitioning. On the one hand, octree representation can eliminate the sparsity in the point cloud. On the other hand, in the voxel domain, convolutions can be naturally expressed, and geometric information (i.e., planes, surfaces, etc.) is explicitly processed by a neural network. Our context model benefits from these properties and learns a probability distribution of the voxels using a deep convolutional neural network with masked filters, called VoxelDNN. Experiments show that our method outperforms the state-of-the-art MPEG G-PCC standard with average rate savings of 28% on a diverse set of point clouds from the Microsoft Voxelized Upper Bodies (MVUB) and MPEG.","URL":"http://arxiv.org/abs/2011.14700","author":[{"family":"Nguyen","given":"Dat Thanh"},{"family":"Quach","given":"Maurice"},{"family":"Valenzise","given":"Giuseppe"},{"family":"Duhamel","given":"Pierre"}],"issued":{"date-parts":[["2020",11,30]]},"fields":{"issued":"2020-11-30","custom":{"links":[{"name":"arXiv","URL":"https://arxiv.org/abs/2011.14700"},{"name":"Source code","URL":"https://github.com/Weafre/VoxelDNN"}],"image":{"src":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAIAAACHqfpvAAAACXBIWXMAAA7DAAAOwwHHb6hkAAABEElEQVQY02P4jwT+/f//998/IAICqMi/f3/+gthPX3/eduz+mw/fwIJQ9Qz/CYG/YM2Hzj8OrNx64dbL/2ALUDRDeC/evj555eKJK5cv3b7x9+9foMjlOy/X7b307fv301dfFLdtuffkHcQ5WGxuXLTGpWa6R82U6qkT//z5DRQ5ev726XNnf/38dOP288OLl//+8QXJLiTNQM/mzNxcUNsUWtmTP2nO/78/gYK/f33/9e3lr5/ff//8/O/X039/viFbBtUM8cakTUfD66f6N8xvWrj+//8//2BhBiS/fH7/8fXdP7++YtEMUfXoyZOLt+5fv3v/4ePHEKf9+vnz1evXL1+9fPv27Zs3b+C+hQAArKmCYzKXLacAAAAASUVORK5CYII=","aspectRatio":2.985074626865672,"src":"/static/9172854c9e1d272bca4b3a9b48903814/f9ff4/2020_11_pcc_lossless_voxeldnn.png","srcSet":"/static/9172854c9e1d272bca4b3a9b48903814/5224a/2020_11_pcc_lossless_voxeldnn.png 200w,\n/static/9172854c9e1d272bca4b3a9b48903814/d786d/2020_11_pcc_lossless_voxeldnn.png 400w,\n/static/9172854c9e1d272bca4b3a9b48903814/f9ff4/2020_11_pcc_lossless_voxeldnn.png 800w,\n/static/9172854c9e1d272bca4b3a9b48903814/3b3e8/2020_11_pcc_lossless_voxeldnn.png 876w","srcWebp":"/static/9172854c9e1d272bca4b3a9b48903814/b0751/2020_11_pcc_lossless_voxeldnn.webp","srcSetWebp":"/static/9172854c9e1d272bca4b3a9b48903814/9e195/2020_11_pcc_lossless_voxeldnn.webp 200w,\n/static/9172854c9e1d272bca4b3a9b48903814/40a1d/2020_11_pcc_lossless_voxeldnn.webp 400w,\n/static/9172854c9e1d272bca4b3a9b48903814/b0751/2020_11_pcc_lossless_voxeldnn.webp 800w,\n/static/9172854c9e1d272bca4b3a9b48903814/1e51c/2020_11_pcc_lossless_voxeldnn.webp 876w","sizes":"(max-width: 800px) 100vw, 800px","presentationWidth":800}}}},"publishedArticle":null}}},{"title":"Improved Deep Point Cloud Geometry Compression","abstract":"Point clouds have been recognized as a crucial data structure for 3D content and are essential in a number of applications such as virtual and mixed reality, autonomous driving, cultural heritage, etc. In this paper, we propose a set of contributions to improve deep point cloud compression, i.e.: using a scale hyperprior model for entropy coding; employing deeper transforms; a different balancing weight in the focal loss; optimal thresholding for decoding; and sequential model training. In addition, we present an extensive ablation study on the impact of each of these factors, in order to provide a better understanding about why they improve RD performance. An optimal combination of the proposed improvements achieves BD-PSNR gains over G-PCC trisoup and octree of 5.51 (6.50) dB and 6.83 (5.85) dB, respectively, when using the point-to-point (point-to-plane) metric.","URL":"http://arxiv.org/abs/2006.09043","author":[{"family":"Quach","given":"Maurice"},{"family":"Valenzise","given":"Giuseppe"},{"family":"Dufaux","given":"Frederic"}],"issued":{"date-parts":[["2020",6,16]]},"fields":{"issued":"2020-06-16","custom":{"links":[{"name":"arXiv","URL":"https://arxiv.org/abs/2006.09043"},{"name":"Source code","URL":"https://github.com/mauriceqch/pcc_geo_cnn_v2"}],"image":{"src":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAA7DAAAOwwHHb6hkAAACWUlEQVQoz0WS3UvTURzGhfKu/oWgLpJC8jaii4rQCCTtRTNEpa4sBAXFKNPsRpMkX8qYlWiQM0svhFAzykynK1/mFs3cfN100225t99eftv89NtZ4Lk45znP8+U5z+H7TUJZG+trbFqtBIMhApKE+c8CXq+PSDSKZmSc5YUlQoEQLqcDy+oKoXCYUCjE5PAYvh03cjhIOBxgd3eXpLhhZ7uKW4VFbG07sG1YKcjNofutOi5ReCaP7BMXCQfCaDXj3MzPRzevJ+CVuJxyjhf3nxCL+BTjBVGftONy8qC8lILrOWzb7Yx8HCDrQjp9vWpmvk9zNPkwmSlncdodvG5r4VJGOgbDHAMdfZxKTqH8yh2iQQ+SdTZhKCvxK8pKeVh1TxCfhobIzc5CPz+H0+Yg7eAxKgpKhaZqe07RjTy8Pg/T37ScPpBK97M3EA3z16hPGMa3pw0NdL1sF8SPKS13lQfsmxtE5AhH9h+iurhKaP2976mprBRYp9Vxct9xxj6MiPumzpgwjMgyqtYmero6BKHVTFBfW411fRXXtouMtPPUltUIra9HTXNDnYJ20Xye4FpqJqP/DZdnzQnDeGf6et/R1txELBbDbFqksb6On1qtSPiopIaSvNuieGz0Ky2NjWwojbOuWCi9Woy6tVPpcox5rXHvyx63my/Dg8q5I0iDbo7pqUmBzUYTrx6r8Ll9yHKi08tmk9D04zMMdvYTVEbol8G4lzC+LGurSJJfYL/Ph0WZTb/fjxSQWDQuirTRaAy7zcb2ll3hA3g8HpZ+m5AjsjKDMTGH/wC70VNH7xV+IgAAAABJRU5ErkJggg==","aspectRatio":2.2222222222222223,"src":"/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/f9ff4/2020_05_pcc_geo_cnn_v2.png","srcSet":"/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/5224a/2020_05_pcc_geo_cnn_v2.png 200w,\n/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/d786d/2020_05_pcc_geo_cnn_v2.png 400w,\n/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/f9ff4/2020_05_pcc_geo_cnn_v2.png 800w,\n/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/73f08/2020_05_pcc_geo_cnn_v2.png 1200w,\n/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/8bb6b/2020_05_pcc_geo_cnn_v2.png 1482w","srcWebp":"/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/b0751/2020_05_pcc_geo_cnn_v2.webp","srcSetWebp":"/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/9e195/2020_05_pcc_geo_cnn_v2.webp 200w,\n/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/40a1d/2020_05_pcc_geo_cnn_v2.webp 400w,\n/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/b0751/2020_05_pcc_geo_cnn_v2.webp 800w,\n/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/a7c53/2020_05_pcc_geo_cnn_v2.webp 1200w,\n/static/c07b8221b44e89ddebe0c9dd8fa9dbd8/ea5df/2020_05_pcc_geo_cnn_v2.webp 1482w","sizes":"(max-width: 800px) 100vw, 800px","presentationWidth":800}}}},"publishedArticle":{"event":"2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)","DOI":"10.1109/MMSP48831.2020.9287077"}}}},{"title":"Folding-based compression of point cloud attributes","abstract":"Existing techniques to compress point cloud attributes leverage either geometric or video-based compression tools. We explore a radically different approach inspired by recent advances in point cloud representation learning. Point clouds can be interpreted as 2D manifolds in 3D space. Specifically, we fold a 2D grid onto a point cloud and we map attributes from the point cloud onto the folded 2D grid using a novel optimized mapping method. This mapping results in an image, which opens a way to apply existing image processing techniques on point cloud attributes. However, as this mapping process is lossy in nature, we propose several strategies to refine it so that attributes can be mapped to the 2D grid with minimal distortion. Moreover, this approach can be flexibly applied to point cloud patches in order to better adapt to local geometric complexity. In this work, we consider point cloud attribute compression; thus, we compress this image with a conventional 2D image codec. Our preliminary results show that the proposed folding-based coding scheme can already reach performance similar to the latest MPEG Geometry-based PCC (G-PCC) codec.","URL":"http://arxiv.org/abs/2002.04439","author":[{"family":"Quach","given":"Maurice"},{"family":"Valenzise","given":"Giuseppe"},{"family":"Dufaux","given":"Frederic"}],"issued":{"date-parts":[["2020",2,11]]},"fields":{"issued":"2020-02-11","custom":{"links":[{"name":"arXiv","URL":"http://arxiv.org/abs/2002.04439"},{"name":"Source code","URL":"https://github.com/mauriceqch/pcc_attr_folding"}],"image":{"src":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAC4jAAAuIwF4pT92AAACAklEQVQoz1WS7U+SYRjF+c/82oe+ud42s0YKSoAlMIk0R9NKrLYyXW3OcJC5bISmhI5KNzVMXmpDpoihLoyWGEwfggcefj1gujxfrus+172z65z7ViBjfs7H7MiTSktRFKvV6x5l5YPrBPf6eR+piL/aU5bYF3I4H1nJ/dyqUlKphKLS+DxvMNXWsJvOcoS+ex10a+oQy8cUPW3NOHqtHFFiScJcf5qZsRfHdxTp9C5mczvK+ivcud7EdixKLL5Bc5MWlVLFQ0srv34k+bwcwNBqplWjZdBmpZD/g8vlRqXSYlA34h7qRyzKG25uJjh3to5LF5VYdA28dAwTCAa4cP4yRn0LtadqsNvteL1e1KoWOtpM3LfewO/30z8wgKpRj1Gr5m6nmbVY/NDy4sIiTucIQl48Xn18fILpmRmyBwLFYpGCnOPo6CuCoRAF2aoon8PhMF1d3byd8rCzkyQRX0NRLv8X0j9IpSKzvmludd6mx6QhElgknxN41v8Yo8GEzdTEVnyVpfk5hocG0ekN6OrOMOF4eigoSdIJwZwgMP/Rx+Skh6uNDXwNBRAO9plwjdFre8A1jZptOarV6ApfQkF8vvfctLTzzjN1aLkiure3x+9MpipYKBTY+JYgk82wtLxMWp5l5Nm6/Fjfk0kW/J+qNRZbJ5VKyb8jTUAWjkSj/AVFsw3G1YgbLwAAAABJRU5ErkJggg==","aspectRatio":2.5316455696202533,"src":"/static/9f720b3fa5032763a50cea91fbb4498f/f9ff4/2020_02_pcc_folding.png","srcSet":"/static/9f720b3fa5032763a50cea91fbb4498f/5224a/2020_02_pcc_folding.png 200w,\n/static/9f720b3fa5032763a50cea91fbb4498f/d786d/2020_02_pcc_folding.png 400w,\n/static/9f720b3fa5032763a50cea91fbb4498f/f9ff4/2020_02_pcc_folding.png 800w,\n/static/9f720b3fa5032763a50cea91fbb4498f/73f08/2020_02_pcc_folding.png 1200w,\n/static/9f720b3fa5032763a50cea91fbb4498f/063af/2020_02_pcc_folding.png 1600w,\n/static/9f720b3fa5032763a50cea91fbb4498f/82865/2020_02_pcc_folding.png 1698w","srcWebp":"/static/9f720b3fa5032763a50cea91fbb4498f/b0751/2020_02_pcc_folding.webp","srcSetWebp":"/static/9f720b3fa5032763a50cea91fbb4498f/9e195/2020_02_pcc_folding.webp 200w,\n/static/9f720b3fa5032763a50cea91fbb4498f/40a1d/2020_02_pcc_folding.webp 400w,\n/static/9f720b3fa5032763a50cea91fbb4498f/b0751/2020_02_pcc_folding.webp 800w,\n/static/9f720b3fa5032763a50cea91fbb4498f/a7c53/2020_02_pcc_folding.webp 1200w,\n/static/9f720b3fa5032763a50cea91fbb4498f/80926/2020_02_pcc_folding.webp 1600w,\n/static/9f720b3fa5032763a50cea91fbb4498f/65305/2020_02_pcc_folding.webp 1698w","sizes":"(max-width: 800px) 100vw, 800px","presentationWidth":800}}}},"publishedArticle":{"event":"2020 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP40778.2020.9191180"}}}},{"title":"Learning Convolutional Transforms for Lossy Point Cloud Geometry Compression","abstract":"Efficient point cloud compression is fundamental to enable the deployment of virtual and mixed reality applications, since the number of points to code can range in the order of millions. In this paper, we present a novel data-driven geometry compression method for static point clouds based on learned convolutional transforms and uniform quantization. We perform joint optimization of both rate and distortion using a trade-off parameter. In addition, we cast the decoding process as a binary classification of the point cloud occupancy map. Our method outperforms the MPEG reference solution in terms of rate-distortion on the Microsoft Voxelized Upper Bodies dataset with 51.5% BDBR savings on average. Moreover, while octree-based methods face exponential diminution of the number of points at low bitrates, our method still produces high resolution outputs even at low bitrates.","URL":"http://arxiv.org/abs/1903.08548","author":[{"family":"Quach","given":"Maurice"},{"family":"Valenzise","given":"Giuseppe"},{"family":"Dufaux","given":"Frederic"}],"issued":{"date-parts":[["2019",3,20]]},"fields":{"issued":"2019-03-20","custom":{"links":[{"name":"arXiv","URL":"https://arxiv.org/abs/1903.08548"},{"name":"Source code","URL":"https://github.com/mauriceqch/pcc_geo_cnn"},{"name":"Supplementary Material Website","URL":"https://mauriceqch.github.io/pcc_geo_cnn_samples/"},{"name":"Supplementary Material","URL":"https://github.com/mauriceqch/pcc_geo_cnn_samples"}],"image":{"src":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAC4jAAAuIwF4pT92AAACEklEQVQoz22QSU9TYRSGv1gkJhrRytTIQiJSxLgjEVMLLLRMkRYqUOZr5woFAi0rl7oiitPGRDf8DH8OQkunW1uGogk8nnuriSF+ycl53/c55yw+lT+oUDj6SaZ8QuHQqAq6dP2o6vOHwg6qmckkr2qDnZD9w4w7hye/UJfmNrHPJgkvRWkKbVHv3+RO/CMdy5+46t+iazpKYHWVa4F3NAizS24XfkV7w+OpOXwrG1wPvufyzGsS299Q6uECjT1uIiMOrMNx1CONGyNxGtzLqN4Qt50uAmMuavqFOTUzN7jqDdLteIDmG8VisC4fsbfbcrCjh7HZCEuROC/jflqdQ9T3PMPWN0Fd1wAz2gtWY4sko/M0dg/SJHm900uLYxhNi5KIhVgLT2O510fs1Qc5qGpx9A4w7p3kfmcbtdZmLLZ2als65D+suPrduJ96uGtvxWK1UXPTzgVbG6quWXZmGHI9obP9FupiHbOL66gh9xjBSIz1xAbe6QXc41M8D0Xwh6KiJ4ktxllZS+DxzeOZqDItGMYz7mM9ucHyWlLYHIMjo3z+8hVVLOrk83kymQx6IU9RL/CjWCSXy4nWyWYzwnMmK8icLtzoBkun07K3T05myuUSx8fHqJ2dHfb29szalUqlUubgX29oIzP97q74fb5LT6XScqRMqVSiJN3QlUoFdXZ2xunpqVn/6vPZeWb4/73fR4HupVHm2twAAAAASUVORK5CYII=","aspectRatio":2.3529411764705883,"src":"/static/9eac4f202f7ec4d87cf376538e329129/f9ff4/2019_pcc_geo_cnn.png","srcSet":"/static/9eac4f202f7ec4d87cf376538e329129/5224a/2019_pcc_geo_cnn.png 200w,\n/static/9eac4f202f7ec4d87cf376538e329129/d786d/2019_pcc_geo_cnn.png 400w,\n/static/9eac4f202f7ec4d87cf376538e329129/f9ff4/2019_pcc_geo_cnn.png 800w,\n/static/9eac4f202f7ec4d87cf376538e329129/73f08/2019_pcc_geo_cnn.png 1200w,\n/static/9eac4f202f7ec4d87cf376538e329129/8cffb/2019_pcc_geo_cnn.png 1280w","srcWebp":"/static/9eac4f202f7ec4d87cf376538e329129/b0751/2019_pcc_geo_cnn.webp","srcSetWebp":"/static/9eac4f202f7ec4d87cf376538e329129/9e195/2019_pcc_geo_cnn.webp 200w,\n/static/9eac4f202f7ec4d87cf376538e329129/40a1d/2019_pcc_geo_cnn.webp 400w,\n/static/9eac4f202f7ec4d87cf376538e329129/b0751/2019_pcc_geo_cnn.webp 800w,\n/static/9eac4f202f7ec4d87cf376538e329129/a7c53/2019_pcc_geo_cnn.webp 1200w,\n/static/9eac4f202f7ec4d87cf376538e329129/e170b/2019_pcc_geo_cnn.webp 1280w","sizes":"(max-width: 800px) 100vw, 800px","presentationWidth":800}}}},"publishedArticle":{"event":"2019 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2019.8803413"}}}}]}},"pageContext":{}},"staticQueryHashes":["1734433022","63159454"]}